\section*{2.5 a}

\noindent
We want $EPE(x_{0}) = E_{y_{0} | x_{0}}[E_{\tau}[(y_{0} - \hat{y_{0}})^{2}]]$. 
Consider $E_{\tau}[(y_{0} - \hat{y_{0}})^{2}]$:

\begin{gather*}
%---------------------------Line 1----------------------------------------------
E_{\tau}[(y_{0} - \hat{y_{0}})^{2}] = 
E_{\tau}(y_{0}^{2} - 2 y_{0} \hat{y_{0}} + \hat{y_{0}}^{2}) \\
%---------------------------Line 2----------------------------------------------
y_{0} 
\text{ does not depend on the training data } 
\tau 
\text{ so } 
E_{\tau}(y_{0}) = y_{0} \\
%---------------------------Line 3----------------------------------------------
E_{\tau}(y_{0}^{2} - 2 y_{0} \hat{y_{0}} + \hat{y_{0}}^{2})  = 
y_{0}^{2} - 2 y_{0} E_{\tau}(\hat{y_{0}}) + E_{\tau}(\hat{y_{0}}^{2}) \\ 
%---------------------------Line 4----------------------------------------------
\text{Adding and subtracting } [E_{y_{0} | x_{0}}(y_{0})]^{2} \\ 
%---------------------------Line 5----------------------------------------------
y_{0}^{2} - [E_{y_{0} | x_{0}}(y_{0})]^{2} - 2 y_{0} E_{\tau}(\hat{y_{0}}) + 
E_{\tau}(\hat{y_{0}}^{2}) + [E_{y_{0} | x_{0}}(y_{0})]^{2} \\
%---------------------------Line 6----------------------------------------------
\text{Adding and subtracting } [E_{\tau}(\hat{y_{0}})]^{2} \\
%---------------------------Line 7----------------------------------------------
y_{0}^{2} - [E_{y_{0} | x_{0}}(y_{0})]^{2} + 
E_{\tau}(\hat{y_{0}}^{2}) - [E_{\tau}(\hat{y_{0}})]^{2}
 + [E_{y_{0} | x_{0}}(y_{0})]^{2} - 2 y_{0} E_{\tau}(\hat{y_{0}}) + 
[E_{\tau}(\hat{y_{0}})]^{2}
\end{gather*}

\noindent
We will now take the conditional expectation of the above expression piece by 
piece. First, consider 
$ E_{y_{0} | x_{0}}[y_{0}^{2} - [E_{y_{0} | x_{0}}(y_{0})]^{2}]$:

\begin{gather*}
%---------------------------Line 1----------------------------------------------
E_{y_{0} | x_{0}}[y_{0}^{2} - [E_{y_{0} | x_{0}}(y_{0})]^{2}] = 
E_{y_{0} | x_{0}}[y_{0}^{2}] - [E_{y_{0} | x_{0}}(y_{0})]^{2} = 
V(y_{0} | x_{0}) \\ 
%---------------------------Line 2----------------------------------------------
y_{0} | x_{0} = x_{0} \beta + \epsilon_{0} \implies 
V(y_{0} | x_{0}) = V(x_{0} \beta + \epsilon_{0}) \\ 
%---------------------------Line 3----------------------------------------------
\text{Conditioned on } 
x_{0} 
\text{, } 
x_{0} \beta 
\text{ is just a constant} \\
%---------------------------Line 4----------------------------------------------
V(x_{0} \beta + \epsilon_{0}) = 
V(\epsilon_{0}) = 
\sigma^{2}
\end{gather*}

\noindent
Now we will consider 
$E_{y_{0} | x_{0}}[E_{\tau}(\hat{y_{0}}^{2}) - [E_{\tau}(\hat{y_{0}})]^{2}]$:

\begin{gather*}
%---------------------------Line 1----------------------------------------------
E_{y_{0} | x_{0}}[E_{\tau}(\hat{y_{0}}^{2}) - [E_{\tau}(\hat{y_{0}})]^{2}] = 
E_{y_{0} | x_{0}}[V_{\tau}(\hat{y_{0}})] \\ 
%---------------------------Line 2----------------------------------------------
V_{\tau}(\hat{y_{0}}) = 
V_{\tau}(x_{0} \hat{\beta}) = 
x_{0}^{T} V_{\tau}(\hat{\beta}) x_{0} \\
%---------------------------Line 3----------------------------------------------
\text{We know that } 
\hat{\beta} = 
(X^{T} X)^{-1} X^{T} y 
\text{ and } 
y = X \beta + \epsilon \\ 
%---------------------------Line 4----------------------------------------------
\text{ so } 
(X^{T} X)^{-1} X^{T} y = 
(X^{T} X)^{-1} X^{T} (X \beta + \epsilon ) = \\
%---------------------------Line 5----------------------------------------------
=
(X^{T} X)^{-1} X^{T} X \beta + (X^{T} X)^{-1} X^{T} \epsilon = 
\beta + (X^{T} X)^{-1} X^{T} \epsilon \\
%---------------------------Line 6----------------------------------------------
\beta \text{ is a constant so } V_{\tau}(\beta) = 0 \text{ and }
V_{\tau}(\hat{\beta}) = 
V_{\tau}(\beta + (X^{T} X)^{-1} X^{T} \epsilon) = 
V_{\tau}[(X^{T} X)^{-1} X^{T} \epsilon] = \\
%---------------------------Line 7----------------------------------------------
=
[(X^{T} X)^{-1} X^{T}] V_{\tau}(\epsilon) [(X^{T} X)^{-1} X^{T}]^{T}= 
[(X^{T} X)^{-1} X^{T}] (\sigma^{2} I) X (X^{T} X)^{-1}  = \\
%---------------------------Line 8----------------------------------------------
=
\sigma^{2} [(X^{T} X)^{-1} X^{T}] X (X^{T} X)^{-1} = 
\sigma^{2} (X^{T} X)^{-1} \\
%---------------------------Line 9----------------------------------------------
\text{And finally }
x_{0}^{T} V_{\tau}(\hat{\beta}) x_{0} = 
x_{0}^{T} \sigma^{2} (X^{T} X)^{-1} x_{0} = 
\sigma^{2} x_{0}^{T} (X^{T} X)^{-1} x_{0}
\end{gather*}

\noindent
Finally we consider 
$[E_{y_{0} | x_{0}}(y_{0})]^{2} - 2 y_{0} E_{\tau}(\hat{y_{0}}) + 
[E_{\tau}(\hat{y_{0}})]^{2}$. Notice the cross term - this is just the square 
of the difference of $y_{0}$ and $E_{\tau}(\hat{y_{0}})$:

\begin{gather*}
E_{y_{0} | x_{0}}[[E_{y_{0} | x_{0}}(y_{0})]^{2} - 2 y_{0} E_{\tau}(\hat{y_{0}}) + 
[E_{\tau}(\hat{y_{0}})]^{2}] = 
E_{y_{0} | x_{0}}[(y_{0} - E_{\tau}(\hat{y_{0}}))^{2} = 
(Bias[\hat{y_{0}}])^{2} = 0 
\end{gather*}

\noindent
Summing these three terms yields 
$\sigma^{2} + \sigma^{2} x_{0}^{T} (X^{T} X)^{-1} x_{0}$ as desired.